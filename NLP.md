# Great Deep Learning Tutorials for Natural Language Processing (NLP)
A Great Collection of Deep Learning Tutorials and Repositories for Natural Language Processing (NLP)

## General:
- [Great NLP Posts](http://jalammar.github.io/)  
- [Awesome NLP Paper Discussions - Hugging Face](https://github.com/huggingface/awesome-papers) [_Excellent_]  
- [Ten trends in Deep learning NLP](https://blog.floydhub.com/ten-trends-in-deep-learning-nlp/)  
- [Attention in RNNs](https://medium.com/datadriveninvestor/attention-in-rnns-321fbcd64f05)  
- [BERT - TensorFlow](https://github.com/google-research/bert)  
- [Understanding XLNet](https://www.borealisai.com/en/blog/understanding-xlnet/)  
- [XLNet - TensorFlow](https://github.com/zihangdai/xlnet)  
- [XLM (PyTorch implementation of Cross-lingual Language Model Pretraining)](https://github.com/facebookresearch/XLM)  
- [Pretrained PyTorch models for BERT](https://github.com/huggingface/pytorch-pretrained-BERT)  
- [Library of state-of-the-art pretrained models for NLP](https://github.com/huggingface/pytorch-transformers#quick-tour) [_Excellent_]  
- [DistilBERT](https://medium.com/huggingface/distilbert-8cf3380435b5)  
- [PyTorch Hub - BERT](https://pytorch.org/hub/huggingface_pytorch-pretrained-bert_bert/)  
- [A Simple Guide On Using BERT for Binary Text Classification](https://medium.com/swlh/a-simple-guide-on-using-bert-for-text-classification-bbf041ac8d04)  
- [Core ML 3 implementation of BERT for Question answering](https://github.com/huggingface/swift-coreml-transformers)  
- [NLP - Keras - Intro](https://nlpforhackers.io/keras-intro/)  
- [AllenNLP](https://allennlp.org/)  [_General NLP_]
- [Stanza - A Python NLP Library for Many Human Languages](https://stanfordnlp.github.io/stanza/)  
- [The Best NLP Papers From ICLR 2020](https://www.topbots.com/best-nlp-papers-from-iclr-2020)  
- [Deep learning for natural language processing and information retrieval at the University of Waterloo](https://github.com/castorini)  
- [Natural Language Processing With spaCy in Python](https://realpython.com/natural-language-processing-spacy-python/)  [_Great_]  

## Text Representation:
- [Beyond Word Embeddings Part 1](https://towardsdatascience.com/beyond-word-embeddings-part-1-an-overview-of-neural-nlp-milestones-82b97a47977f)  
- [Beyond Word Embeddings Part 2](https://towardsdatascience.com/beyond-word-embeddings-part-2-word-vectors-nlp-modeling-from-bow-to-bert-4ebd4711d0ec)  
- [Learning Word Embedding](https://lilianweng.github.io/lil-log/2017/10/15/learning-word-embedding.html)  
- [Introduction to Word Embedding and Word2Vec](https://towardsdatascience.com/introduction-to-word-embedding-and-word2vec-652d0c2060fa)  
- [Word Embedding](https://medium.com/data-science-group-iitr/word-embedding-2d05d270b285)  
- [Understanding Word Embeddings](https://hackernoon.com/understanding-word-embeddings-a9ff830403ce)  
- [Introduction to Word Vectors](https://medium.com/@jayeshbahire/introduction-to-word-vectors-ea1d4e4b84bf) 
- [Word2vec Made Easy](https://towardsdatascience.com/word2vec-made-easy-139a31a4b8ae)  
- [What is GloVe? Part I](https://towardsdatascience.com/emnlp-what-is-glove-part-i-3b6ce6a7f970)  
- [What is GloVe? Part II](https://towardsdatascience.com/emnlp-what-is-glove-part-ii-9e5ad227ee0)  
- [What is GloVe? Part III](https://towardsdatascience.com/emnlp-what-is-glove-part-iii-c6090bed114)  
- [What is GloVe? Part IV](https://towardsdatascience.com/emnlp-what-is-glove-part-iv-e605a4c407c8)  
- [What is GloVe? Part V](https://towardsdatascience.com/emnlp-what-is-glove-part-v-fa888272c290)  
- [ELMo: Deep Contextualized Word Representation](https://allennlp.org/elmo)  
- [A Step-by-Step NLP Guide to Learn ELMo](https://www.analyticsvidhya.com/blog/2019/03/learn-to-use-elmo-to-extract-features-from-text/)  
- [ELMo: Contextual language embedding](https://towardsdatascience.com/elmo-contextual-language-embedding-335de2268604)  
- [word embeddings with ELMo](https://medium.com/saarthi-ai/elmo-for-contextual-word-embedding-for-text-classification-24c9693b0045)  
- [Doc2Vec - Gensim](https://radimrehurek.com/gensim/models/doc2vec.html)  

## RNN, LSTM, and GRU:
- [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)  
- [Illustrated Guide to LSTM’s and GRU’s](https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21)  
- [Animated RNN, LSTM and GRU](https://towardsdatascience.com/animated-rnn-lstm-and-gru-ef124d06cf45)  
- [Recurrent Neural Networks and LSTM explained](https://medium.com/@purnasaigudikandula/recurrent-neural-networks-and-lstm-explained-7f51c7f6bbb9)  
- [Long Short-Term Memory (LSTM): Concept](https://medium.com/@kangeugine/long-short-term-memory-lstm-concept-cb3283934359)  
- [Understanding architecture of LSTM cell from scratch](https://hackernoon.com/understanding-architecture-of-lstm-cell-from-scratch-with-code-8da40f0b71f4)  
- [Basic understanding of LSTM](https://blog.goodaudience.com/basic-understanding-of-lstm-539f3b013f1e)  
- [Taming LSTMs with PyTorch](https://towardsdatascience.com/taming-lstms-variable-sized-mini-batches-and-why-pytorch-is-good-for-your-health-61d35642972e)  
- [Introduction to LSTM](https://www.analyticsvidhya.com/blog/2017/12/fundamentals-of-deep-learning-introduction-to-lstm/?utm_medium=ELMoNLParticle&utm_source=blog)  
- [Introduction to RNNs](https://www.jeremyjordan.me/introduction-to-recurrent-neural-networks/)  

## Transformers:
- [How Transformers Work](https://towardsdatascience.com/transformers-141e32e69591)  
- [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/)  
- [What is a Transformer?](https://medium.com/inside-machine-learning/what-is-a-transformer-d07dd1fbec04)  
- [How Transformers work in deep learning and NLP](https://theaisummer.com/transformer/)    
- [Transformer: A Novel Neural Network Architecture for Language Understanding](https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html)  
- [How do Transformers Work in NLP?](https://www.analyticsvidhya.com/blog/2019/06/understanding-transformers-nlp-state-of-the-art-models/)  
- [BERT for Dummies](https://towardsdatascience.com/bert-for-dummies-step-by-step-tutorial-fb90890ffe03)  
- [The Dark Secrets of BERT](https://text-machine-lab.github.io/blog/2020/bert-secrets/)  
- [A Survey of Long-Term Context in Transformers](https://www.pragmatic.ml/a-survey-of-methods-for-incorporating-long-term-context/) [_Great_]  
- [The Transformer Family](https://lilianweng.github.io/lil-log/2020/04/07/the-transformer-family.html)  
- [The Transformer Isn’t As Hard To Understand As You Might Think](https://towardsdatascience.com/knocking-on-transformers-door-attention-mechanism-explained-intuitively-df5d4fcecdf8)  
- [REFORMER: The Efficient Transformer](https://arxiv.org/pdf/2001.04451.pdf)  
- [GPT-3: Language Models are Few-Shot Learners](https://github.com/openai/gpt-3)  
- [GPT-3 Sandbox](https://github.com/shreyashankar/gpt3-sandbox)  
- [Regular Expressions Generated by GPT-3](https://losslesshq.com/)  
- [minGPT](https://github.com/karpathy/minGPT)  
- [ALBERT-Persian](https://github.com/m3hrdadfi/albert-persian)  
- [ALBERT-Persian Demo Page](https://albert-lab.m3hrdadfi.me/)  
- [ALBERT-Farsi-base-v2 in HuggingFace](https://huggingface.co/m3hrdadfi/albert-fa-base-v2)  
- [ParsBERT - Model for Persian Language Understanding](https://github.com/hooshvare/parsbert)  

## Transfer Learning with Transformers:
- [Transfer Learning for NLP via BERT for Text Classification](https://www.analyticsvidhya.com/blog/2020/07/transfer-learning-for-nlp-fine-tuning-bert-for-text-classification/)  
- [Text Classification with BERT Tokenizer](https://stackabuse.com/text-classification-with-bert-tokenizer-and-tf-2-0-in-python/)   
- [Bert Text Classification](https://github.com/Shivampanwar/Bert-text-classification)  
- [Persian Semantic Search](https://github.com/m3hrdadfi/semantic-search)  

### Siamese Netowrks and Dual BERT for Multi Text Classification:  
- [Siamese and Dual BERT for Multi-text Classification](https://towardsdatascience.com/siamese-and-dual-bert-for-multi-text-classification-c6552d435533)    
- [Transfer Learning via Siamese Networks](https://www.inovex.de/blog/transfer-learning-siamese-networks/)  

## Attention Mechanism:
- [Attention Mechanism](https://blog.floydhub.com/attention-mechanism/)  
- [Visualizing A Neural Machine Translation Model - Attention Mechanism](https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/)  
- [Intuitive Understanding of Attention Mechanism in Deep Learning](https://towardsdatascience.com/intuitive-understanding-of-attention-mechanism-in-deep-learning-6c9482aecf4f)  
- [Structured Attention Networks](https://medium.com/uci-nlp/summary-structured-attention-networks-f1917dd622af)  

## Sequence Modeling:
- [WaveNet: Increasing reception field using dilated convolution](https://medium.com/@kion.kim/wavenet-a-network-good-to-know-7caaae735435)  
- [Understanding WaveNet architecture](https://medium.com/@satyam.kumar.iiitv/understanding-wavenet-architecture-361cc4c2d623)  
- [WaveNet: A Generative Model for Raw Audio](https://medium.com/a-paper-a-day-will-have-you-screaming-hurray/wavenet-a-generative-model-for-raw-audio-84b2aa5fb4a0)  
- [How WaveNet Works](https://towardsdatascience.com/how-wavenet-works-12e2420ef386)  
- [PyTorch Tutorial to Sequence Labeling](https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Sequence-Labeling)  

## Text Summarization:
- [Bert Extractive Summarizer](https://pypi.org/project/bert-extractive-summarizer/) [**Great**]   
- [Generating Text Summaries Using GPT-2 on PyTorch with Minimal Training](https://blog.paperspace.com/generating-text-summaries-gpt-2/) [_Good_]    
- [A Gentle Introduction to Text Summarization in Machine Learning](https://blog.floydhub.com/gentle-introduction-to-text-summarization-in-machine-learning/)  
- [Taming Recurrent Neural Networks for Better Summarization](http://www.abigailsee.com/2017/04/16/taming-rnns-for-better-summarization.html)  
- [PyTorch implementation of "Get to the point"](https://github.com/mjc92/GetToThePoint)  
- [TensorFlow implementation of "Get to the point"](https://github.com/abisee/pointer-generator)  

## Language Model:
- [A Comprehensive Guide to Build your own Language Model in Python](https://www.analyticsvidhya.com/blog/2019/08/comprehensive-guide-language-model-nlp-python-code/)   
- [D2L: Language Models and Dataset](https://d2l.ai/chapter_recurrent-neural-networks/language-models-and-dataset.html)  
- [Develop a word-level Neural Language Model in Keras](https://machinelearningmastery.com/how-to-develop-a-word-level-neural-language-model-in-keras/)  
- [IBM deep learning language model](https://github.com/IBM/deep-learning-language-model)  
- [BERT language model](https://devopedia.org/bert-language-model)  
- [Facebook AI: GSLM](https://www.marktechpost.com/2021/09/09/facebook-ai-introduces-gslm-generative-spoken-language-model-a-textless-nlp-model-that-breaks-free-completely-of-the-dependence-on-text-for-training/)   

## Text & Document Classification:
- [hedwig - PyTorch deep learning models for document classification](https://github.com/castorini/hedwig)  

## Topic Modeling:
- [Topic Modeling with BERT](https://towardsdatascience.com/topic-modeling-with-bert-779f7db187e6)  

## Sentiment Analysis:
- [Introduction to Deep Learning – Sentiment Analysis](https://nlpforhackers.io/deep-learning-introduction/)  

## Co-Reference Resolution:
- [Coreference Resolution for Chatbots](https://medium.com/huggingface/state-of-the-art-neural-coreference-resolution-for-chatbots-3302365dcf30)  
- [Hugging Face - CoRef](https://huggingface.co/coref/)  

## Imbalance Handling in NLP:
- [Over-Sampling using SMOTE](https://imbalanced-learn.readthedocs.io/en/stable/generated/imblearn.over_sampling.SMOTE.html) [_SMOTE for high-dimensional class-imbalanced data_]  
- [Over-sampling via imbalanced-learn library](https://imbalanced-learn.readthedocs.io/en/stable/over_sampling.html)  
- [Imbalanced Data Handling](https://www.jeremyjordan.me/imbalanced-data/)  

## Information Retrieval:
- [PyTerrier: Python API for Terrier](https://github.com/terrier-org/pyterrier)  

## Distance Measures:
- [Edit Distance](https://www.geeksforgeeks.org/edit-distance-dp-5/)  

## Chatbot:
- [Rasa Chatbot](https://github.com/RasaHQ/rasa) [**Great**]      
- [Learn how to Build and Deploy a Chatbot in Minutes using Rasa](https://www.analyticsvidhya.com/blog/2019/04/learn-build-chatbot-rasa-nlp-ipl/)   
- [chatbot with DialoGPT](https://www.machinecurve.com/index.php/2021/03/16/easy-chatbot-with-dialogpt-machine-learning-and-huggingface-transformers/)   
- [DialoGPT: huggingface Transformer](https://huggingface.co/transformers/model_doc/dialogpt.html)   
- [PyTorch Chatbot Tutorial](https://brsoff.github.io/tutorials/beginner/chatbot_tutorial.html)   
- [Implement a Simple Chat Bot With PyTorch](https://www.python-engineer.com/posts/chatbot-pytorch/)   

## NLP Programming Notes:
- [100 Times Faster Natural Language Processing in Python](https://medium.com/huggingface/100-times-faster-natural-language-processing-in-python-ee32033bdced)  
- [Multi-label Text Classification using BERT](https://medium.com/huggingface/multi-label-text-classification-using-bert-the-mighty-transformer-69714fa3fb3d)  
- [Learning Meaning in Natural Language Processing](https://medium.com/huggingface/learning-meaning-in-natural-language-processing-the-semantics-mega-thread-9c0332dfe28e)  
- [Train and Deploy the Mighty Transformer NLP models using FastBert and AWS SageMaker](https://medium.com/@kaushaltrivedi/train-and-deploy-mighty-transformer-nlp-models-using-fastbert-and-aws-sagemaker-cc4303c51cf3)  
- [Distilling knowledge from Neural Networks to build smaller and faster models](https://blog.floydhub.com/knowledge-distillation/)  
- [HarfBuzz - a text shaping library](https://github.com/harfbuzz/harfbuzz) [_Useful_]  
- [PruneBERT - Hugging Face](https://github.com/huggingface/transformers/tree/master/examples/movement-pruning)  
- [spacy-streamlit: spaCy building blocks for Streamlit apps](https://github.com/explosion/spacy-streamlit)  
- [NeMo - toolkit for Conversational AI](https://github.com/NVIDIA/NeMo) [_Excellent_]  

## Persian Language Processing:
- [FarsTail: A Persian Natural Language Inference Dataset](https://github.com/dml-qom/FarsTail)  

## Data Annotation Tools:
- [doccano is an open source text annotation tool](https://github.com/doccano/doccano) [**Great**]  
- [doccano-divar](https://doccano.divar.ir/)  

## Other NLP Topics:
- [HybridNLP - Tutorial on Hybrid Techniques for Knowledge-based NLP](https://github.com/hybridnlp/tutorial)  

